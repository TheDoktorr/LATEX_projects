    \documentclass[11pt]{article}
    \usepackage{amsmath}
    \usepackage{graphicx}
    \usepackage{multirow}
    \usepackage{booktabs}
    \usepackage{subfigure}
    \usepackage{verbatim}
    \usepackage{color}
    \usepackage{hyperref}
    \usepackage{url}
    \usepackage[version=3]{mhchem}
    \usepackage{svg}
    \usepackage[raggedright]{sidecap}
    \usepackage[margin=1in]{geometry}

\geometry{
 a4paper,
 left=2.54cm,
 right=2.54cm,
 bottom=2.54cm,
 top=2.54cm, 
 headsep=0.1cm,
 }

    \begin{document}

\title{\vspace{-1cm}Statistically modelling the random nature of beta decay in Uranium-238}
    \author{Andrew Hamill 39047415}
    \date{\today}
    \maketitle
    \section{abstract}
Radioactive sources decay randomly over time, with a fixed probability. The half life describes the time taken for a sample to halve its count rate (\cite{YF}). The count rate over a short period of time can be modelled by statistical distributions, most commonly Poisson (\cite{campbell2008poisson},\cite{buczyk2009poisson}), in order to model the spread of decays and calculate the respective standard deviation from this. The experimentally obtained values of standard deviation and count rate for the source, U-238, were used to test a Poisson fit and compare the two values of standard deviation. Here it was shown $\sigma_{data} = 1.71$ (3 sf.). Based on the model of random error and the estimated Poisson parameter, it was shown that $\sigma_{Po.2} = 1.80$ (3 sf.).
     
\noindent These two values of standard deviation are significantly close, suggesting a good fit to the experimental data, which was confirmed by a $\chi^{2}$ Goodness of Fit test. Thus it was concluded that beta decay of U-238 is random and can be successfully modelled by a Poisson distribution. This is reflective of the generally accepted model of radioactive decays (\cite{YF}), which appear random over single time intervals. This opens up questions for larger time periods, as well as the validity of the model that require a deep understanding of stats and further research in the area.
    \section{Introduction}
    % general background
Unstable elements undergo radioactive decay over their lifetime in order to reach stability (\cite{YF}). This occurs through 3 main types of decay, alpha, beta and gamma, represented by their Greek alphabet symbols, $\alpha,\  \beta,\ \gamma$.  These describe emission of \ce{^{4}_{2}He}, \ce{^{0}_{-1}e}, and \ce{^{0}_{0}\gamma} respectively from the nucleus of the unstable element, with a sequential combination of these decays present until a stable state is reached (\cite{Magill2005}). The decay of these elements can occur over a long period of time, and to measure and compare this the half life of an element is investigated. This describes the time it takes for the count rate, $R$, of a substance to fall to half its initial value (\cite{YF}). This is unique to the element and its decay.
    % specific background 
    
\noindent These decays are random within an arbitrary short time period as a result of the fixed probability of decay. The half life of an element can be hard to accurately measure due to their significant length, for example, the half life of \ce{^{238}_{}U} is (4.4683 ± 0.0096) Ga (\cite{U238}).  It can be achieved from taking readings over several time periods, and solving the equations for exponential decay (\cite{YF}); however the resolution of this method suffers in insufficiently long time periods. Instead, the probability of decay can be modelled from several repeated smaller time periods. This can be modelled statistically to determine the  specific parameters of an element's decay. Statistical models such as Poisson are especially effective as they describe a discrete rate (in this case, decay) and have a flexible distribution that can be fit to a spread of data, making it ideal to model the random nature of radioactive decay in a small time period (\cite{thompson2001poisson}). 
    
\noindent This experiment aimed to investigate the random nature of beta decay, specifically for elements with a substantial half life. Following this, the aim was to test how it can be modelled by a Poisson distribution and how the Poisson's parameters compared to the statistics of the raw experimental data. The difference between them, as well as the overall shape of the modelled distribution, provide quantitative and qualitative analysis on the Goodness of Fit of the Poisson in this case. This is further extended with the inclusion of a Chi squared Goodness of Fit test, to determine whether the fit is statistically significant.  We further analysed the background count and activity to determine the contribution of background radiation to our data. 
    
\noindent Here it was shown $\sigma_{data} = 1.71$ (3 sf.) and $\sigma_{Po.2} = 1.80$ (3 sf.) from our experimental data. Our experimental result was within five percent of the model and so it was deemed this a good model for our data. Additionally, the Chi squared value $p = 0.820$ (3 sf.) suggests the model is an appropriate fit for our data. Experimental standard deviation is also within five percent of our model before considering sources of error, suggesting the model is a good fit to our data. 
    
\noindent Additionally the experiment helped understanding of data modelling, specifically understanding the conditions for which statistical models can be used as well as their shortcomings. Much of this work involves a deep understanding of the statistics as well as the physics, as these complex distributions are often valid under very strict circumstances, and approximations have to be made and evaluated whether they are sensible or not, in order for these models to be accurately used.
\noindent As such, the importance of strict scientific procedure in order to reduce error and accurately model data is evident here. One must ensure approximations do not tarnish the quality of experimental data collected as well as the validity of the model, e.g. here the time period used, to ensure it can be said with confidence whether or not a model is valid. 
    \section{Fundamentals of radioactive decay and Poisson modelling}
Though only mentioned briefly, the half life relationship is defined as,
\begin{equation}
        t_{\frac{1}{2}} = \frac{\ln{2}}{\lambda}
    \end{equation}
where $ t_{\frac{1}{2}}$ is the time for the number of particles to fall to half of its original value (or the count rate), and $\lambda$ is the decay constant: the probability per decay.\cite{YF}) \\
These elements are in an initial state of instability, and they decay to a more stable state. The nature of the instability and the structure of the atomic nuclei determines the type of the decay. For example, elements with a larger number of neutrons compared to protons generally decay via $\beta^{-}$ decay, hence a neutron will decay into a proton with the release of an electron and electron antineutrino. \\
There are many types of fundamental decays, Alpha $\alpha$, Beta plus/minus $\beta^{-}/\beta^{+}$, Gamma, $\gamma$, as well as electron capture and instantaneous fission (\cite{Magill2005}) to name the most understood ones. This report is only concerned with $\beta^{-}$ decay, defined by the following equation,
    \begin{equation}
    \ce{^{238}_{92}U} \longrightarrow \ce{^{234}_{90}Th} + \ce{^{4}_{2}\alpha} + \ce{^{0}_{0}\gamma} \longrightarrow \ce{^{234}_{91}Pa} + \ce{^{0}_{-1}\beta^{-}} + \ce{^{0}_{0}\bar{\nu_{e}}}
\end{equation}
where U is Uranium, Th is Thorium and Pa is Protactinium. Other particles released include the 3 decays mentioned above, as well as an electron antineutrino, $\ce{^{0}_{0}\bar{\nu_{e}}}$. Protactinium also further decays by $\beta$, however this equation is omitted here (\cite{arazo2016uranium}). \\
   %This can also be analysed on a quark level, producing the follow quark equation:
% \newline
%  \begin{equation} \ce{^{1}_{0}n} \longrightarrow %\ce{^{1}_{1}p} + \ce{^{0}_{-1}e} + \ce{^{0}_{0}%\bar{\nu_{e}}} 
%  \end{equation}
%\newline
%This describes the emission of an electron and how a neutron changes to a proton, taking an element with excess neutrons closer towards stability. This occurs through lots of decays over a large time period. 
% The decay we are studying is the $\beta^{-}$ decay of \ce{^{238}U}. This has an approximate half life of $4 x 10^9$ years REF. Why is this a beta emitter?
In order to calculate the count rate, which is the rate at which the source decays per time interval (here seconds, $s$), 
  \begin{equation} 
  R = \frac{N}{T} \pm \frac{\sqrt{N}}{T}
   \end{equation}
where $R$ is the count rate, $T$ is the total time interval, $N$ is the total counts. This follows fairly intuitively from the definition. \\
The Poisson distribution (\cite{hu2008poisson}) will be used to model the data. The defining equation for the distribution is, 
    \begin{equation}
        P(N) = \frac{e^{-m}m^{n}}{N!}
    \end{equation}
where $P$ is the probability of obtaining $N$ counts, and $m$ is the mean. Notice the factorial in the definition, indicating that an integer input must be used throughout as the distribution is discrete. \\
    As the radioactive decay is a random variable distribution, the mean, $m$ and standard deviation $\sigma$ are defined by,
    \begin{equation}
        \sigma = \sqrt{m}
    \end{equation}
This will be used to compare against the mean of the raw data according to the histogram. This is computed as the mean of the whole data array. 
    \section{Experimental exploration of the random nature of radioactive decay}
   % \centering
   % \scalebox{0.5}
\begin{figure}[h]
        \begin{center}
            \def\svgwidth{\columnwidth}
            \includesvg[height=7.0cm]{final_diag.svg}
             \caption{A diagram of the experimental setup, in which a U-238 source is placed with tweezers on blue holding tray, set at distance d from the Geiger-Müller tube. This is connected to the shielded housing in order to ensure the distance d and angle relative to the source are held constant. This tube is connected to an electronic counter, used here to take  100 three second readings of counts from the $\beta$ decay of the source.}
             \label{fig:experimental setup}
             \vspace{-1.5em}
        \end{center}
    \end{figure}
\noindent Before taking readings of the radioactive source, any present background radiation had to be considered. Though the source was placed in a somewhat shielded housing (see Figure 1), sources of background radiation are present everywhere and so a flat background count rate is first established before introducing more potent sources. To do this, the source was kept in a lead lined storage container, at a significant distance from a Geiger-Müller tube, connected normal to a shielded housing that will hold the source. This detector is then connected to an electric counter, responsible for counting the number of electric pulses received in a set time interval.

\noindent To measure the background count, the number of decayed nuclei was measured in an interval of 100 seconds. The background activity, or counts per second in Bequerel (Bq), could then be calculated using the relationship given in Equation 3. The time interval of 100 seconds was chosen as it is significantly larger than the interval used to investigate random decay from the source, as well as being a convenient number for calculation purposes.

\noindent In order to test the random fluctuations, the \ce{^{238}U} source was carefully placed at a distance from the Geiger-Müller tube such that the count rate would be $<$10 for any three second interval. This ensures the conditions for a Poisson model (\cite{thompson2001poisson}) are best met, namely that events are "rare" within the time interval, in effect ensuring they are discrete and countable, as well as each decay being independent. In testing this was roughly 8cm. As the probability for larger count rates falls quickly towards zero for increasing distances, and the Poisson conditions must be valid for the model to produce a good fit, this distance was not varied much. If the Poisson fit poorly this could be further investigated.

\noindent The count rate in three-second intervals was then measured 100 times, ensuring the timer and counter was reset each measurement and the source was kept at uniform distance and angle from the detector. This was achieved by placing the source on one of the adjustable shelves in the protective housing, which ensured the source was shielded when working with it for longer time periods, as well as providing a stand for the Geiger-Müller tube to ensure this did not move. The raw data was collected and processed accordingly by computer software. This data was collected all at once, ensuring minimal environment changes.
\section{Analysis of decay parameters and fitting the statistical model}
The data was plotted on a histogram (Fig. 3) with integer bins, and error bars calculated based on the square root of the individual bin frequencies as this is the error in random variables.  The overall shape is positively skewed around a mean approximated to be between 3 and 4.

\noindent To further analyse the data, the software calculated the mean of the data, $3.25$, and standard deviation, $\sigma_{data} = 1.71 $ (3 sf.)
The mean of the histogram was also considered, where the bin centers are considered instead of the integer values. This gave us a slightly different standard deviation, $\sigma_{hist} = 1.94$ (3 sf.). 

\noindent Following this, Python and packages NumPy and SciPy were used to create a suitable Poisson fit to the data. Two different methods were explored, SciPy's Curve fit and Stats fit. 
Figure 2 shows the overlaid Poisson curve from Stats fit on top of the histogram. The histogram has been adjusted with bin centres as the integers associated with each reading, rather than the convention of a bin starting and ending at each integer. This was a necessary compromise due to the Poisson equation (Equation 4) requiring integer inputs, and the curve was fit to the centres of the bins, in order to better approximate the distribution shape. The curve was also smoothed using a basic Spline function, in order to qualitatively evaluate the overall fit better, under the understanding this creates a continuous curve from a discrete distribution.
\begin{figure}[h]
        \begin{center}
            \def\svgwidth{\columnwidth}
            \includesvg[height=8.0cm]{histogram_new.svg}
             \caption{“graph illustrates SCiPy's Stats fit Poisson curve, overlaid with the histogram of counts in a three second interval. The histogram has been adjusted to have bin centres as integers in order to fit the Poisson, and the errors in the histogram are based on the error of random variables.The Poisson's output standard deviation was calculated as $\sigma_{Po.2} = 1.80$ (3 sf.). by the Stats fit,  based on the input range for m, $[3,4]$ as well as a Poisson function."}
             \label{fig:experimental results 1}
             \vspace{-1.5em}
        \end{center}
    \end{figure}
\noindent The curve fit function generated a suggested mean, and from this a standard deviation (using Equation 5) $\sigma_{Po.1} = 1.62$ (3 sf.). The Stats fit function similarly generated a standard deviation $\sigma_{Po.2} = 1.80 $ (3 sf.). Both functions took a Poisson function as the fit function, and Curve fit requested a guess for mean (3.25 was used) while Stats fit asked for bounds for the mean, where $[3,4]$ was given. The Curve fit procedure seemed erroneous as this was significantly shifted, going through none of the error bars so this was not used in the results. 

\noindent Additionally, the count rate was also calculated (Equation 3), to be $R = 1.08 \pm 0.06$ Bq (3 sf.), with a background rate of $0.160$ Bq (3 sf.). From this, it can be inferred 15 percent of the counts may be associated with background radiation and, as such this is a source of random error in the Poisson fit.
A Goodness of Fit test for the Stats fit Poisson curve generated $p = 0.820$ (3 sf.), which indicates the curve is a good fit to the data (see Appendix for full test). This confirms the hypothesis that a Poisson would be a good model for random radioactive decay. 

\noindent The variation between $\sigma_{Po.2}$ and $\sigma_{data}$ is likely explained by the background radiation as well as the inherent randomness in radioactive decay. The experimental data varied from the model by around five percent, and with the background count suggested it could be up to 15 percent of the total count rate it can be inferred this is likely the case. Taking 100 data readings yielded an error of roughly $\sqrt{N} = 10$ percent is observed, therefore testing with larger data sets would likely reduce some of this error.  

\noindent The use of a three-second time interval here must be considered, as for this data the model can be said to fit. However, the choice of this time interval is fairly arbitrary, choosing to just be significantly less. Further investigation should be carried out to see how it behaves in the cases as the time interval approaches the half life. It is known for these larger values of time that the model is actually closer to a binomial model (\cite{SITEK20151105}), and at what point the approximation changes, as well as what changes in modelling assumptions requires further research.
    %discussion of the results in context with future work
    \section{Conclusions}
The aim of the experiment was to investigate whether radioactive decay could be modelled by a random discrete distribution, such as Poisson. This was achieved through taking readings for background radiation, followed by 100 three-second intervals for which the count was observed. This data was plotted on a histogram and a Poisson curve was fit over it in Python. Here it was shown $\sigma_{data} = 1.71$ and $\sigma_{Po.2} = 1.80$ (3 sf.). The variation in these is due to the background count, as well as the random variation present. This would be minimised through increased testing. The Poisson curve was also deemed a good fit for the data through a $\chi^{2}$ Goodness of Fit test with a value $p = 0.820$ (3 sf.). Therefore it is  concluded the decay can be modelled by a discrete random variable and the use of a Poisson distribution is suitable to model this data graphically.

\noindent This result is important as it helps increase understanding of the nature of radioactive decay, especially its behaviour in smaller time intervals where it can be easily analysed. More generally, it outlines the general procedure for choosing and fitting a model to some data, whilst demonstrating the compromises and adjustments required to do this. The results were limited by the amount of data collected, and this could be further investigated with a large data set, to see if the relationship holds for large counts. Additional research into larger time periods would broaden our understanding of when the Poisson model breaks down and what it would evolve into, as well as the assumptions made for each model considered and how the physics is able to justify these. To challenge the accepted models is to further scientific understanding.

    \section{References}
\bibliographystyle{vancouver} 
\bibliography{refs} % Entries are in the refs.bib file
    \section{Appendices}
    Calculating $R$, $$ R = \frac{N}{T} \pm \frac{\sqrt{N}}{T}$$
  Where $R$ is the count rate, $T$ is the total time interval and $N$ is the total counts. With values $N =( 32.5 \pm 2) * 10^{1}$, $T= 3 * 100 [s]$, gives 
  $$R = 0.92 \pm 0.06  Bq$$ 
    \begin{table}[h]
        \centering
        \begin{tabular}{|c|c|}
        \hline
          \textbf{Count}  & \textbf{Frequency} \\
          \hline
           0  & 1 \\
           1 & 14\\
           2  & 25 \\
           3  & 18 \\
           4  & 20 \\
           5  & 11 \\
           6  & 6\\
           7  & 4 \\
           8  & 1 \\
           \hline
        \end{tabular}
        \caption{Binned data}
        \label{tab:my_label}
    \end{table}
    Poisson values 
    $N = 60$, and $x$ is the counts in 100s. 
    \begin{table}[h]
        \centering
        \begin{tabular}{|c|c|c|c|}
         \hline
           counts (n)  & Poisson probability P(x=n) & $ f_{e}, P(x=n) * N$ & $f_{0}$ \\
            \hline
            0 & 0.0387742 & 3.87742 & 1 \\
            1 & 0.12601617 & 12.601617 & 14\\
            2 & 0.20477628 & 20.477628 & 25\\
            3 & 0.22184097 & 22.184097 & 18\\
            4 & 0.18024579 & 18.024579 & 20\\
            5 & 0.11715976 & 11.715976 & 11\\
            6 & 0.06346154 & 6.346154 & 6 \\
            7 & 0.02946428 & 2.946428 & 4\\
            8 & 0.01196986 & 1.1996986 & 1\\
             \hline
        \end{tabular}
        \caption{Raw Poisson data for GoF}
        \label{tab:my_label}
    \end{table}
    Following this, we combine frequencies less than 4 as they are too small for GoF. 
    \begin{table}[h]
        \centering
        \begin{tabular}{|c|c|c|c|c|}
         \hline
           counts (n)  & Poisson probability P(x=n) & $ f_{e}, P(x=n) * N$ & $f_{0}$ & $\chi^{2}$ Contribution\\
 \hline
            <1 & 0.16479038 & 16.479038 & 15 &0.1327476401\\
            2 & 0.20477628 & 20.477628 & 25&0.9987410899\\
            3 & 0.22184097 & 22.184097 & 18&0.7891539469\\
            4 & 0.18024579 & 18.024579 & 20&0.2164981566\\
            5 & 0.11715976 & 11.715976 & 11&0.04375406988\\
            6 & 0.06346154 & 6.346154 & 6 &0.01888113521\\
            >7 & 0.04772526 & 4.772526 & 5 &0.01084214537\\      
 \hline
        \end{tabular}
        \caption{Corrected Frequencies and Poisson Contributions}
        \label{tab:my_label}
    \end{table}
    In order to calculate $\chi^{2}$, we firstly estimate the degrees of freedom, df. This dataset has 7 entries, and 1 estimated parameter, $m$ therefore, 
    $$df = 7 - 1 - 1 = 5$$
    We then use the equation for $\chi^{2}$, 
    $$\chi^{2} = \sum{\frac{(f_{O}-f_{e})^{2}}{f_{e}}}$$
    To generate a $\chi^{2} = 2.21061818$ and $p = 0.81930076$. Using the standard hypothesis,
    \newline
    $H_{0} =$ The Poisson is a good fit for the data
    \newline
    $H_{1} =$ The Poisson is not a good fit for the data
    \newline
    As our p value is >0.05, it is significant at the 
    five percent level, deeming it a good fit for our data. 
     \end{document}